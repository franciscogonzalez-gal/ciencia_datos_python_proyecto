{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-05T02:29:16.440375Z",
     "start_time": "2025-03-05T02:29:14.049726Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class ProyectoRegresionLinealCSV:\n",
    "    def __init__(self, ruta_csv, nombre_col_y):\n",
    "        \"\"\"\n",
    "        Constructor de la clase.\n",
    "        Carga el dataset desde un archivo CSV y separa en entrenamiento (80%) y validación (20%).\n",
    "\n",
    "        Parámetros:\n",
    "        ruta_csv (str): Ruta al archivo CSV con los datos.\n",
    "        nombre_col_y (str): Nombre de la columna que se usará como variable dependiente (Y).\n",
    "        \"\"\"\n",
    "        # Cargamos el dataset completo en un DataFrame\n",
    "        self.df = pd.read_csv(ruta_csv)\n",
    "\n",
    "        # Mezclamos aleatoriamente las filas del DataFrame (opcional, pero recomendable)\n",
    "        self.df = self.df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Verificamos que la columna Y exista\n",
    "        if nombre_col_y not in self.df.columns:\n",
    "            raise ValueError(f\"La columna '{nombre_col_y}' no se encuentra en el CSV.\")\n",
    "\n",
    "        # Guardamos el nombre de la columna Y\n",
    "        self.col_y = nombre_col_y\n",
    "\n",
    "        # Dividimos en entrenamiento (80%) y validación (20%)\n",
    "        n_filas = len(self.df)\n",
    "        self.n_train = int(0.8 * n_filas)\n",
    "\n",
    "        self.df_entrenamiento = self.df.iloc[:self.n_train, :].copy()\n",
    "        self.df_validacion    = self.df.iloc[self.n_train:, :].copy()\n",
    "\n",
    "        # El resto de columnas (excepto Y) se considerarán como potenciales X\n",
    "        self.columnas_X = [c for c in self.df.columns if c != self.col_y]\n",
    "\n",
    "        # Aquí almacenaremos las dos mejores variables independientes\n",
    "        self.mejores_vars = []\n",
    "\n",
    "    def analisis_exploratorio(self):\n",
    "        \"\"\"\n",
    "        Realiza el análisis exploratorio de los datos de entrenamiento:\n",
    "        - Calcula media, min, max, rango y desviación estándar (std).\n",
    "        - Genera histogramas usando seaborn distplot para cada columna (X y la Y).\n",
    "        \"\"\"\n",
    "        print(\"=== Análisis Exploratorio (Entrenamiento) ===\")\n",
    "        df_desc = self.df_entrenamiento.describe().T\n",
    "        df_desc[\"rango\"] = df_desc[\"max\"] - df_desc[\"min\"]\n",
    "        print(df_desc[[\"mean\", \"min\", \"max\", \"rango\", \"std\"]])\n",
    "\n",
    "        # Graficar histogramas de todas las columnas\n",
    "        for col in self.df_entrenamiento.columns:\n",
    "            plt.figure()\n",
    "            sns.distplot(self.df_entrenamiento[col], kde=True, hist=True)\n",
    "            plt.title(f\"Histograma de {col}\")\n",
    "            plt.show()\n",
    "\n",
    "    def encontrar_mejores_variables(self):\n",
    "        \"\"\"\n",
    "        Calcula la correlación entre cada posible X y la Y en el set de entrenamiento.\n",
    "        Selecciona las 2 de mayor |correlación|.\n",
    "        Muestra scatterplots de esas relaciones X vs. Y.\n",
    "        \"\"\"\n",
    "        # Convertimos a numpy para cálculo de correlación, o podemos usar df.corr()\n",
    "        # Usaremos np.corrcoef de forma individual para ilustración.\n",
    "        df_train = self.df_entrenamiento\n",
    "\n",
    "        y = df_train[self.col_y].values\n",
    "\n",
    "        correlaciones = {}\n",
    "        for col in self.columnas_X:\n",
    "            # Verificamos que la columna sea numérica antes de calcular correlación\n",
    "            if pd.api.types.is_numeric_dtype(df_train[col]):\n",
    "                x = df_train[col].values\n",
    "                corr_val = np.corrcoef(x, y)[0, 1]\n",
    "                correlaciones[col] = corr_val\n",
    "\n",
    "                # Scatter plot\n",
    "                plt.figure()\n",
    "                plt.scatter(x, y)\n",
    "                plt.title(f\"{col} vs {self.col_y} (corr = {corr_val:.4f})\")\n",
    "                plt.xlabel(col)\n",
    "                plt.ylabel(self.col_y)\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"La columna '{col}' no es numérica. Se omite de la correlación.\")\n",
    "\n",
    "        # Ordenar según correlación absoluta\n",
    "        correlaciones_ordenadas = sorted(correlaciones.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "        # Tomar las 2 variables con mayor correlación\n",
    "        if len(correlaciones_ordenadas) < 2:\n",
    "            raise ValueError(\"No hay suficientes variables numéricas para seleccionar las 2 mejores.\")\n",
    "\n",
    "        self.mejores_vars = [correlaciones_ordenadas[0][0], correlaciones_ordenadas[1][0]]\n",
    "\n",
    "        print(\"\\nLas dos variables con mayor correlación con\", self.col_y, \"son:\")\n",
    "        print(self.mejores_vars)\n",
    "\n",
    "    def entrenar_modelo_gradiente_desc(self, x, y, epochs=100, alpha=0.001, imprimir_error_cada=10):\n",
    "        \"\"\"\n",
    "        Entrena un modelo de regresión lineal univariable y = b0 + b1*x de forma manual con gradiente descendente.\n",
    "\n",
    "        Devuelve:\n",
    "         - historial_modelos: dict donde la llave es la iteración y el valor es [b1, b0].\n",
    "         - errores: lista con el error (MSE/2) en cada iteración.\n",
    "        \"\"\"\n",
    "        n = len(x)\n",
    "        # Matriz X_aug -> [x, 1]\n",
    "        X_aug = np.column_stack((x, np.ones(n)))\n",
    "\n",
    "        # Inicializamos b1, b0\n",
    "        b1, b0 = 0.0, 0.0\n",
    "        theta = np.array([b1, b0])  # [b1, b0]\n",
    "\n",
    "        historial_modelos = {}\n",
    "        errores = []\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            y_hat = X_aug @ theta  # (n,) = b1*x + b0\n",
    "            # Error (MSE/2)\n",
    "            error = (1/(2*n)) * np.sum((y_hat - y)**2)\n",
    "            errores.append(error)\n",
    "\n",
    "            # Gradiente vectorizado: dE/dtheta = (1/n) * X_aug^T (y_hat - y)\n",
    "            grad = (1/n) * (X_aug.T @ (y_hat - y))\n",
    "\n",
    "            # Actualizar theta\n",
    "            theta = theta - alpha * grad\n",
    "\n",
    "            # Guardamos\n",
    "            historial_modelos[epoch] = np.copy(theta)\n",
    "\n",
    "            if epoch % imprimir_error_cada == 0:\n",
    "                print(f\"Iteración {epoch} - Error: {error:.6f}\")\n",
    "\n",
    "        return historial_modelos, errores\n",
    "\n",
    "    def graficar_error(self, errores):\n",
    "        \"\"\"\n",
    "        Grafica la evolución del error (MSE/2) a lo largo de las iteraciones.\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, len(errores)+1), errores)\n",
    "        plt.xlabel(\"Iteración\")\n",
    "        plt.ylabel(\"Error (MSE/2)\")\n",
    "        plt.title(\"Evolución del Error durante el Entrenamiento\")\n",
    "        plt.show()\n",
    "\n",
    "    def graficar_evolucion_modelo(self, x, y, historial_modelos, n_saltos=10):\n",
    "        \"\"\"\n",
    "        Muestra cómo evoluciona la recta del modelo conforme avanzan las iteraciones.\n",
    "\n",
    "        Parámetros:\n",
    "        x, y (np.array): Datos de entrenamiento para graficar.\n",
    "        historial_modelos (dict): {iter -> [b1, b0]}.\n",
    "        n_saltos (int): Frecuencia de iteraciones a mostrar.\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        plt.scatter(x, y, label=\"Datos de entrenamiento\")\n",
    "\n",
    "        max_iter = max(historial_modelos.keys())\n",
    "        iteraciones_interes = range(n_saltos, max_iter+1, n_saltos)\n",
    "\n",
    "        for i in iteraciones_interes:\n",
    "            b1, b0 = historial_modelos[i]\n",
    "            y_pred = b1 * x + b0\n",
    "            plt.plot(x, y_pred, label=f\"Iter {i}\")\n",
    "\n",
    "        # Última recta\n",
    "        b1_final, b0_final = historial_modelos[max_iter]\n",
    "        y_pred_final = b1_final * x + b0_final\n",
    "        plt.plot(x, y_pred_final, label=f\"Final (Iter {max_iter})\", linewidth=2)\n",
    "\n",
    "        plt.title(\"Evolución del Modelo en el Tiempo\")\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def entrenar_modelo_sklearn(self, x, y):\n",
    "        \"\"\"\n",
    "        Entrena un modelo de regresión lineal univariable con scikit-learn.\n",
    "        Retorna la instancia de LinearRegression entrenada.\n",
    "        \"\"\"\n",
    "        X_2d = x.reshape(-1, 1)  # scikit-learn requiere 2D\n",
    "        modelo = LinearRegression()\n",
    "        modelo.fit(X_2d, y)\n",
    "        return modelo\n",
    "\n",
    "    def predecir_combinado(self, x_nuevo, theta_manual, modelo_sklearn):\n",
    "        \"\"\"\n",
    "        Dado un vector x_nuevo, devuelve 3 vectores:\n",
    "         1) y_hat_manual (con los parámetros manuales)\n",
    "         2) y_hat_sklearn (con el modelo sklearn)\n",
    "         3) y_hat_promedio (promedio de las dos predicciones anteriores).\n",
    "        \"\"\"\n",
    "        b1, b0 = theta_manual  # [b1, b0]\n",
    "        y_hat_manual = b1*x_nuevo + b0\n",
    "\n",
    "        X_2d = x_nuevo.reshape(-1, 1)\n",
    "        y_hat_sklearn = modelo_sklearn.predict(X_2d)\n",
    "\n",
    "        y_hat_promedio = (y_hat_manual + y_hat_sklearn) / 2\n",
    "        return y_hat_manual, y_hat_sklearn, y_hat_promedio\n",
    "\n",
    "    def error_mse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcula el MSE (Mean Squared Error) entre y_true y y_pred.\n",
    "        \"\"\"\n",
    "        return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "    def ejecutar_proyecto(self, epochs=200, alpha=0.001, imprimir_error_cada=20):\n",
    "        \"\"\"\n",
    "        Orquesta todo el proceso:\n",
    "        1) Análisis exploratorio\n",
    "        2) Selección de 2 mejores variables X basadas en correlación\n",
    "        3) Entrenamiento manual y con sklearn\n",
    "        4) Validación final en set de prueba\n",
    "        5) Gráfica de comparación de errores\n",
    "        \"\"\"\n",
    "        # 1) Análisis Exploratorio\n",
    "        print(\"========== (1) Análisis Exploratorio ==========\")\n",
    "        self.analisis_exploratorio()\n",
    "\n",
    "        # 2) Encontrar 2 mejores variables\n",
    "        print(\"========== (2) Seleccionar Mejores Variables ==========\")\n",
    "        self.encontrar_mejores_variables()\n",
    "\n",
    "        # Preparar DataFrames de train y val\n",
    "        df_train = self.df_entrenamiento\n",
    "        df_val   = self.df_validacion\n",
    "\n",
    "        # y de entrenamiento y validación\n",
    "        y_train = df_train[self.col_y].values\n",
    "        y_val   = df_val[self.col_y].values\n",
    "\n",
    "        errores_finales = {\n",
    "            \"var\": [],\n",
    "            \"error_manual\": [],\n",
    "            \"error_sklearn\": [],\n",
    "            \"error_promedio\": []\n",
    "        }\n",
    "\n",
    "        for var in self.mejores_vars:\n",
    "            print(f\"\\n=== Entrenando con variable: {var} ===\")\n",
    "            x_train = df_train[var].values\n",
    "            x_val   = df_val[var].values\n",
    "\n",
    "            # (3) Entrenar modelo manual\n",
    "            historial_modelos, errores = self.entrenar_modelo_gradiente_desc(\n",
    "                x_train, y_train,\n",
    "                epochs=epochs,\n",
    "                alpha=alpha,\n",
    "                imprimir_error_cada=imprimir_error_cada\n",
    "            )\n",
    "\n",
    "            # Graficar evolución del error\n",
    "            self.graficar_error(errores)\n",
    "\n",
    "            # Graficar evolución de la recta\n",
    "            self.graficar_evolucion_modelo(x_train, y_train, historial_modelos, n_saltos=epochs//5)\n",
    "\n",
    "            # Modelo final (última iteración)\n",
    "            b1_final, b0_final = historial_modelos[epochs]\n",
    "            print(f\"Modelo Manual Final: y = {b1_final:.4f} * x + {b0_final:.4f}\")\n",
    "\n",
    "            # (3) Entrenar modelo sklearn\n",
    "            modelo_sklearn = self.entrenar_modelo_sklearn(x_train, y_train)\n",
    "            print(f\"Modelo sklearn: y = {modelo_sklearn.coef_[0]:.4f} * x + {modelo_sklearn.intercept_:.4f}\")\n",
    "\n",
    "            # (4) Validación\n",
    "            y_hat_manual, y_hat_sklearn, y_hat_promedio = self.predecir_combinado(\n",
    "                x_val,\n",
    "                np.array([b1_final, b0_final]),\n",
    "                modelo_sklearn\n",
    "            )\n",
    "\n",
    "            error_mse_manual   = self.error_mse(y_val, y_hat_manual)\n",
    "            error_mse_sklearn  = self.error_mse(y_val, y_hat_sklearn)\n",
    "            error_mse_promedio = self.error_mse(y_val, y_hat_promedio)\n",
    "\n",
    "            print(f\"Error validación (MSE) - Manual:   {error_mse_manual:.6f}\")\n",
    "            print(f\"Error validación (MSE) - sklearn:  {error_mse_sklearn:.6f}\")\n",
    "            print(f\"Error validación (MSE) - Promedio: {error_mse_promedio:.6f}\")\n",
    "\n",
    "            # Guardar para comparación\n",
    "            errores_finales[\"var\"].append(var)\n",
    "            errores_finales[\"error_manual\"].append(error_mse_manual)\n",
    "            errores_finales[\"error_sklearn\"].append(error_mse_sklearn)\n",
    "            errores_finales[\"error_promedio\"].append(error_mse_promedio)\n",
    "\n",
    "        # (5) Gráfico comparativo de errores\n",
    "        df_errores = pd.DataFrame(errores_finales)\n",
    "\n",
    "        plt.figure()\n",
    "        x_pos = np.arange(len(df_errores))\n",
    "        bar_width = 0.2\n",
    "\n",
    "        plt.bar(x_pos - bar_width,\n",
    "                df_errores[\"error_manual\"],\n",
    "                width=bar_width,\n",
    "                label=\"Manual\")\n",
    "\n",
    "        plt.bar(x_pos,\n",
    "                df_errores[\"error_sklearn\"],\n",
    "                width=bar_width,\n",
    "                label=\"sklearn\")\n",
    "\n",
    "        plt.bar(x_pos + bar_width,\n",
    "                df_errores[\"error_promedio\"],\n",
    "                width=bar_width,\n",
    "                label=\"Promedio\")\n",
    "\n",
    "        plt.xticks(x_pos, df_errores[\"var\"])\n",
    "        plt.ylabel(\"MSE (validación)\")\n",
    "        plt.title(\"Comparación de Errores en Validación\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Conclusiones\n",
    "        print(\"\\n======== CONCLUSIÓN FINAL ========\")\n",
    "        for idx, row in df_errores.iterrows():\n",
    "            var_ = row[\"var\"]\n",
    "            min_error = min(row[\"error_manual\"], row[\"error_sklearn\"], row[\"error_promedio\"])\n",
    "            if min_error == row[\"error_manual\"]:\n",
    "                mejor_modelo = \"Manual\"\n",
    "            elif min_error == row[\"error_sklearn\"]:\n",
    "                mejor_modelo = \"sklearn\"\n",
    "            else:\n",
    "                mejor_modelo = \"Promedio\"\n",
    "            print(f\"Para la variable '{var_}' el mejor modelo es: {mejor_modelo}, con MSE = {min_error:.6f}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:29:18.113512Z",
     "start_time": "2025-03-05T02:29:16.446704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# EJEMPLO DE USO (Fuera de la clase, en tu script principal o Jupyter Notebook):\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Ajusta la ruta al CSV y el nombre de la columna que será tu Y (ej. \"adr\", \"children\", etc.)\n",
    "    ruta_csv = \"hotel_bookings.csv\"\n",
    "    columna_objetivo = \"adr\"  # Ejemplo: predecir 'adr' (Average Daily Rate), cambia según tus necesidades\n",
    "\n",
    "    proyecto = ProyectoRegresionLinealCSV(ruta_csv, columna_objetivo)\n",
    "\n",
    "    # Ejecutar todo el pipeline\n",
    "    proyecto.ejecutar_proyecto(\n",
    "        epochs=200,\n",
    "        alpha=0.001,\n",
    "        imprimir_error_cada=20\n",
    "    )"
   ],
   "id": "427c7f2a9aa0df8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== (1) Análisis Exploratorio ==========\n",
      "=== Análisis Exploratorio (Entrenamiento) ===\n",
      "                                       mean      min     max   rango  \\\n",
      "is_canceled                        0.371367     0.00     1.0    1.00   \n",
      "lead_time                        104.209345     0.00   737.0  737.00   \n",
      "arrival_date_year               2016.158315  2015.00  2017.0    2.00   \n",
      "arrival_date_week_number          27.169047     1.00    53.0   52.00   \n",
      "arrival_date_day_of_month         15.805354     1.00    31.0   30.00   \n",
      "stays_in_weekend_nights            0.927014     0.00    19.0   19.00   \n",
      "stays_in_week_nights               2.497581     0.00    50.0   50.00   \n",
      "adults                             1.855379     0.00    55.0   55.00   \n",
      "children                           0.103370     0.00    10.0   10.00   \n",
      "babies                             0.008020     0.00     9.0    9.00   \n",
      "is_repeated_guest                  0.031724     0.00     1.0    1.00   \n",
      "previous_cancellations             0.087015     0.00    26.0   26.00   \n",
      "previous_bookings_not_canceled     0.135575     0.00    72.0   72.00   \n",
      "booking_changes                    0.222862     0.00    21.0   21.00   \n",
      "agent                             86.486400     1.00   535.0  534.00   \n",
      "company                          189.187454     6.00   543.0  537.00   \n",
      "days_in_waiting_list               2.257392     0.00   391.0  391.00   \n",
      "adr                              101.777337    -6.38   510.0  516.38   \n",
      "required_car_parking_spaces        0.062254     0.00     8.0    8.00   \n",
      "total_of_special_requests          0.571457     0.00     5.0    5.00   \n",
      "\n",
      "                                       std  \n",
      "is_canceled                       0.483173  \n",
      "lead_time                       107.122590  \n",
      "arrival_date_year                 0.707829  \n",
      "arrival_date_week_number         13.599292  \n",
      "arrival_date_day_of_month         8.777035  \n",
      "stays_in_weekend_nights           0.996514  \n",
      "stays_in_week_nights              1.902596  \n",
      "adults                            0.548583  \n",
      "children                          0.397595  \n",
      "babies                            0.094772  \n",
      "is_repeated_guest                 0.175265  \n",
      "previous_cancellations            0.844210  \n",
      "previous_bookings_not_canceled    1.490188  \n",
      "booking_changes                   0.653588  \n",
      "agent                           110.625238  \n",
      "company                         132.222261  \n",
      "days_in_waiting_list             17.146687  \n",
      "adr                              48.001893  \n",
      "required_car_parking_spaces       0.245273  \n",
      "total_of_special_requests         0.793424  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fjgon\\AppData\\Local\\Temp\\ipykernel_40748\\611708902.py:57: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot(self.df_entrenamiento[col], kde=True, hist=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Resort Hotel'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m proyecto \u001B[38;5;241m=\u001B[39m ProyectoRegresionLinealCSV(ruta_csv, columna_objetivo)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Ejecutar todo el pipeline\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m proyecto\u001B[38;5;241m.\u001B[39mejecutar_proyecto(\n\u001B[0;32m     13\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m,\n\u001B[0;32m     14\u001B[0m     alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m,\n\u001B[0;32m     15\u001B[0m     imprimir_error_cada\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m\n\u001B[0;32m     16\u001B[0m )\n",
      "Cell \u001B[1;32mIn[1], line 227\u001B[0m, in \u001B[0;36mProyectoRegresionLinealCSV.ejecutar_proyecto\u001B[1;34m(self, epochs, alpha, imprimir_error_cada)\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;66;03m# 1) Análisis Exploratorio\u001B[39;00m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m========== (1) Análisis Exploratorio ==========\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 227\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manalisis_exploratorio()\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# 2) Encontrar 2 mejores variables\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m========== (2) Seleccionar Mejores Variables ==========\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 57\u001B[0m, in \u001B[0;36mProyectoRegresionLinealCSV.analisis_exploratorio\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_entrenamiento\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m     56\u001B[0m     plt\u001B[38;5;241m.\u001B[39mfigure()\n\u001B[1;32m---> 57\u001B[0m     sns\u001B[38;5;241m.\u001B[39mdistplot(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf_entrenamiento[col], kde\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, hist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     58\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHistograma de \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     59\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Ciencia_Datos\\Lib\\site-packages\\seaborn\\distributions.py:2443\u001B[0m, in \u001B[0;36mdistplot\u001B[1;34m(a, bins, hist, kde, rug, fit, hist_kws, kde_kws, rug_kws, fit_kws, color, vertical, norm_hist, axlabel, label, ax, x)\u001B[0m\n\u001B[0;32m   2440\u001B[0m     a \u001B[38;5;241m=\u001B[39m x\n\u001B[0;32m   2442\u001B[0m \u001B[38;5;66;03m# Make a a 1-d float array\u001B[39;00m\n\u001B[1;32m-> 2443\u001B[0m a \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(a, \u001B[38;5;28mfloat\u001B[39m)\n\u001B[0;32m   2444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m a\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   2445\u001B[0m     a \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39msqueeze()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Ciencia_Datos\\Lib\\site-packages\\pandas\\core\\series.py:1031\u001B[0m, in \u001B[0;36mSeries.__array__\u001B[1;34m(self, dtype, copy)\u001B[0m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;124;03mReturn the values as a NumPy array.\u001B[39;00m\n\u001B[0;32m    983\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1028\u001B[0m \u001B[38;5;124;03m      dtype='datetime64[ns]')\u001B[39;00m\n\u001B[0;32m   1029\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1030\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m-> 1031\u001B[0m arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(values, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[0;32m   1032\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_copy_on_write() \u001B[38;5;129;01mand\u001B[39;00m astype_is_view(values\u001B[38;5;241m.\u001B[39mdtype, arr\u001B[38;5;241m.\u001B[39mdtype):\n\u001B[0;32m   1033\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mview()\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'Resort Hotel'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
